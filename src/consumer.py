#!/usr/bin/python

from pymongo import MongoClient
import urllib
import urllib2
import re
import httplib
import requests
import urlparse
import configparser
import sys
import sqlite3
import datetime

class SafebrowsinglookupClient(object):
    def __init__(self, key='', debug=0, error=0):
        """ Create a new client. You must pass your Google API key (http://code.google.com/apis/safebrowsing/key_signup.html).

            Arguments:
                key: API key.
                debug: Set to 1 to print debug & error output to the standard output. 0 (disabled) by default.
                error: Set to 1 to print error output to the standard output. 0 (disabled) by default.
        """
        self.key = key
        self.debug = debug
        self.error = error
        self.last_error = ''
        self.version = '0.2'
        self.api_version = '3.1'

        if self.key == '':
            raise ValueError("Missing API key")


    def lookup(self, *urls):
        """ Lookup a list of URLs against the Google Safe Browsing v2 lists.

            Returns a hash <url>: <Gooogle match>. The possible values for <Gooogle match> are: "ok" (no match), "malware", "phishing", "malware,phishing" (match both lists) and "error".

            Arguments:
                urls: List of URLs to lookup. The Lookup API allows only 10,000 URL checks a day. If you need more, use the official Google Safe Browsing v2 API implementation (http://code.google.com/p/google-safe-browsing/downloads/list). Each requests must contain 500 URLs at most. The lookup() method will split the list of URLS in blocks of 500 URLs if needed.
        """
        results = {}
        count = 0
        while count * 500 < len(urls):
            inputs = urls[count * 500 : (count + 1) * 500]
            body = len(inputs)

            for url in inputs:
                body = str(body) + "\n" + self.__canonical(str(url))

            self.__debug("BODY:\n" + body + "\n\n")
            #url = 'https://sb-ssl.google.com/safebrowsing/api/lookup?client=%s&key=%s&appver=%s&pver=%s' % ('python', self.key, self.version, self.api_version)
            url = 'https://sb-ssl.google.com/safebrowsing/api/lookup?client=api&apikey=%s&appver=1.0&pver=3.0' % (self.key)
            self.__debug("URL: %s" % (url))

            response = ''
            try:
                response = urllib2.urlopen(url, body)
            except Exception, e:
                if hasattr(e, 'code') and e.code == httplib.NO_CONTENT: # 204
                    self.__debug("No match\n")
                    results.update( self.__ok(inputs) )

                elif hasattr(e, 'code') and e.code == httplib.BAD_REQUEST: # 400
                    self.__error("Invalid request")
                    results.update( self.__errors(inputs) )

                elif hasattr(e, 'code') and e.code == httplib.UNAUTHORIZED: # 401
                    self.__error("Invalid API key")
                    results.update( self.__errors(inputs) )

                elif hasattr(e, 'code') and e.code == httplib.FORBIDDEN: # 403 (should be 401)
                    self.__error("Invalid API key")
                    results.update( self.__errors(inputs) )

                elif hasattr(e, 'code') and e.code == httplib.SERVICE_UNAVAILABLE: # 503
                    self.__error("Server error, client may have sent too many requests")
                    results.update( self.__errors(inputs) )

                else:
                    self.__error("Unexpected server response")
                    self.__debug(e)
                    results.update( self.__errors(inputs) )
            else:
                response_read = response.read()
                if not response_read:
                    self.__debug("No match\n")
                    results.update( self.__ok(inputs) )
                else:
                    self.__debug("At least 1 match\n")
                    results.update( self.__parse(response_read.strip(), inputs) )

            count = count + 1

        return results



    # Private methods

    # Not much is actually done, full URL canonicalization is not required with the Lookup library according to the API documentation
    def __canonical(self, url=''):
        # remove leading/ending white spaces
        url = url.strip()

        # Remove any embedded tabs and CR/LF characters which aren't escaped.
        url = url.replace('\t', '').replace('\r', '').replace('\n', '')

        # make sure whe have a scheme
        scheme = re.compile("https?\:\/\/", re.IGNORECASE)
        if scheme.match(url) is None:
            url = "http://" + url

        return url


    def __parse(self, response, urls):
        lines = response.splitlines()

        if (len(urls) != len(lines)):
            self.__error("Number of URLs in the response does not match the number of URLs in the request");
            self.__debug( str(len(urls)) + " / " + str(len(lines)) )
            self.__debug(response);
            return self.__errors(urls);


        results = { }
        for i in range(0, len(lines)):
            results.update({urls[i] : lines[i]})

        return results


    def __errors(self, urls):
        results = {}
        for url in urls:
            results.update({url: 'error'})

        return results


    def __ok(self, urls):
        results = {}
        for url in urls:
            results.update({url: 'ok'})

        return results


    def __debug(self, message=''):
        if self.debug == 1:
            print message


    def __error(self, message=''):
        if self.debug == 1 or self.error == 1:
            print message + "\n"
            self.last_error = message

#end of class for Google Safe Browsing

#create Domains Dict
domainDict = {}
domainFile = open("data/top-1m.csv", "r")
lines = domainFile.readlines()
for line in lines:
    line = line.rsplit()
    line = line[0]
    #print(line)
    domainDict[str(line.split(',')[0])] = line.split(',')[1:][0]
domainFile.close()



##########GLOBALS###################
config = configparser.ConfigParser()
config.read('consumer-settings.cfg')
consumer_key = config.get('googlesafebrowsing', 'consumer_key')
verySafeDomainList = ['http://twitter.com', 'https://twitter.com', 'http://globo.com']

total_steps = 7
steps = total_steps * [0]

client = MongoClient()
db = client.dbtesting
cursor = db.tweets.find()

#google safe browsing
#mykey = "thekey"
sf = SafebrowsinglookupClient(consumer_key)
#sf = None
#result = sf.lookup("www.gumblar.cn")
#print(result)
#result = sf.lookup("www.uol.com.br")
#print(result)
#sys.exit(1)

################STEPS#################
#step1: GoogleSageBrowsing
#returns 1 if url is marked 'malware'
def step1(myurl):
    res = sf.lookup(myurl)
    if res[myurl] == 'malware':
        return 1
    else:
        return 0

#Search url in Domain list
#returns 1 if domain is not on the list
def step2(url):
    print("step2 received: " + url)
    domain = str(urlparse.urljoin(url, "/")[7:-1])
    if domain.startswith("/"):  #http or https
        domain = domain[1:]
        if domain.startswith("www."):
            domain = domain[4:]

        res = domain in domainDict.values()
        if res:
            #it's on the list
            return 0
        else:
            return 1

    return 1

###### STORING MALICIOUS URL ON DB (sqlite) ###########
def store_malicious(url, data):
    try:
        con = sqlite3.connect('/home/emanuel/Dropbox/usp-022015/progweb/projeto-final/src/malware.db')
        cur = con.cursor()
        #cur.executemany('INSERT INTO urls VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        cur.execute('INSERT INTO urls VALUES(NULL, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        con.commit()
        con.close()

        #cur.execute("select count(*) from contacts")
    except sqlite3.OperationalError, err:
        print("SQLITE error!")
        print(err)
        pass
        #cur.execute("create table contacts(name varchar(20), address varchar(50), phone varchar(30))")


for document in cursor:
    try:
        URLList= re.findall(r'http://[a-zA-Z0-9\.\/\?&=_]+', document[u'text'])

        for shorted_url in URLList:
            urlIsSafe = False
            #unshort url
            try:
                url = str(requests.head(shorted_url, allow_redirects=True).url)
            except requests.exceptions.ConnectionError:
                continue

            #Maybe it's a very old url
            if url is None:
                continue

            #very well-kknown domains
            for safeDomain in verySafeDomainList:
                if url.startswith(safeDomain):
                    urlIsSafe = True

            #Our steps..
            if urlIsSafe is False:
                #step1 - GoogleSageBrowsing
                print("Submetting: " + url)
                steps[0] = step1(url)
                print("step1: " + url + " -> " + str(steps[0]))

                #step2 - Search on Domain List
                steps[1] = step2(url)
                print("step2: " + url + " -> " + str(steps[1]))

                mytimestamp = (document[u'created_at'] - datetime.datetime(1970, 1, 1)).total_seconds()

                try:
                    mylocation = document[u'location']
                except KeyError:
                    mylocation = "NULL"


                data = (str(document[u'_id']), int(mytimestamp), "aa", str(url), mylocation) + tuple(steps) + (1,)
                #print(data)
                #if any step is 1 we store it.
                for step in steps:
                    if step == 1:
                        store_malicious(url, data)
                        break
                #print(document)

    except KeyError:
        pass

